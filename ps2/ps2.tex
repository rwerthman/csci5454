\documentclass[12pt]{article}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\textwidth}{6.5in}
\setlength{\parindent}{0in}
\setlength{\parskip}{\baselineskip}

\usepackage{amsmath,amsfonts,amssymb,graphicx,enumerate}

\begin{document}

CSCI 5454 \hfill Problem Set 2\\
Robert Werthman\\
No Collaborators

\hrulefill

\begin{enumerate}

	\item \textit{Probability boot camp}
	
	\begin{enumerate}
	
		\item \textit{Prove Markov's inequality, $Pr[X \ge c] \le E[X]/c$, with $c>0$}\\
		\\
		The formula for the probability of a continuous random variable $X$ with probability density function $f(x)$ is
			$$
			Pr[x_1 \le X \le x_2] = \int_{x_1}^{x_2} f(x)dx
			$$	
		And the formula for the expected value of a continuous random variable $X$ with probability density function $f(x)$ is
			$$
			E[x_1 \le X \le x_2] = \int_{x_1}^{x_2} xf(x)
			$$	
		so for $Pr[X \ge c]$ we have
			$$
			Pr[X \ge c] = \int_{c}^{\infty} f(x)dx
			$$
		and since $X$ is a nonnegative random variable we have
			$$
			E[X] = \int_{0}^{\infty} xf(x)dx
			$$
		Notice that $0 < c \le \infty$.  This tells us that the bounds of $E[X]$ are greater than $Pr[X \ge c]$.  We can break up the integral formed by $E[X]$ to create an inequality that 
		will begin to look similar to the integral of $Pr[X \ge c]$.
			\begin{align*}
				E[X] &= \int_{0}^{\infty} xf(x)dx\\
				&= \int_{0}^{c} xf(x)dx + \int_{c}^{\infty} xf(x)dx\\
				&\ge \int_{c}^{\infty} xf(x)dx\\
			\end{align*}
		We can assume that $x \ge c$ because $c$ is one of the bounds of the integral.  This means we can substitute $c$ for $x$.
			\begin{align*}
				 \int_{c}^{\infty} xf(x)dx \ge \int_{c}^{\infty} cf(x)dx \ge c\int_{c}^{\infty} f(x)dx
			\end{align*}
		We now have an equation for $E[X]$ that has $Pr[X \ge c]$.
			$$
			E[X] \ge c\int_{c}^{\infty} f(x)dx = cPr[X \ge c]
			$$
		Dividing both sides by $c$ gives us
			$$
			E[X]/c \ge Pr[X \ge c]
			$$
		which is Markov's inequality.  We have just shown that $Pr[X \ge c] \le E[X]/c$, with $c>0$ is true based on the probability and expected value of the continuous random variable $X$.
		
		\newpage
		\item \textit{Prove Chebyshev's inequality $Pr[|X-\mu| \ge c \cdot \sigma] \le 1/c^2$}\\
		\\
		If we let $k = (c \cdot \sigma)$ we have
			$$
			Pr[|X-\mu| \ge c \cdot \sigma] = Pr[|X-\mu| \ge k]
			$$
		One of the properties of $|a|$ is that it can also be represented as $\sqrt{a^2}$ so we can change\\
			$$
			Pr[|X-\mu| \ge k] = Pr[\sqrt{(X-\mu)^2} \ge k]
			$$
		If we take the square root of both sides of the inequality we get
			$$
			Pr[(X-\mu)^2 \ge k^2]
			$$
		From here we can use Markov's inequality since we know $(X-\mu)^2$ is nonnegative.  If we let $(X-\mu)^2 = X$ and $k^2 = c$ and substitute those into Markov's inequality we have 
			$$
			Pr[(X-\mu)^2 \ge k^2] \le E[(X-\mu)^2]/k
			$$
		The variance $\sigma^2$ of a continuous random variable $X$ with mean $\mu$ is 
			$$
			\sigma^2 = E[(X-\mu)^2]
			$$
		So
			$$
			Pr[(X-\mu)^2 \ge k^2] \le E[(X-\mu)^2]/k
			$$
		Now becomes
			$$
			Pr[(X-\mu)^2 \ge k^2] \le \sigma^2/k
			$$
		If we now change $k$ back to $c \cdot \sigma$ we have the equation
			$$
			Pr[(X-\mu)^2 \ge (c \cdot \sigma)^2] \le \sigma^2/c^2 \cdot \sigma^2
			$$
		Reducing the right side of the equation results in
			$$
			Pr[(X-\mu)^2 \ge (c \cdot \sigma)^2] \le 1/c^2
			$$
		And taking the square root of both sides of the inequality on the left hand side of the equation results in
			$$
			Pr[\sqrt{(X-\mu)^2} \ge c \cdot \sigma] \le 1/c^2
			$$ 
		
		Which can be further reduced to 
			$$
			Pr[|X-\mu| \ge c \cdot \sigma] \le 1/c^2
			$$ 
		Which is Chebyshev's inequality.  We have just shown that Chebyshev's inequality can be proven using Markov's Inequality.
		\newpage
		\item \textit{Show that for any discrete random variables $X,X',E[X] = E[E[X|X']]$.}\\
		\\
		If we let $Y = X'$ we have
			$$
			E[X] = E[E[X|Y]]
			$$
		The expected value of a discrete random variable X is
			$$
			E[X] = \sum_{x} x \cdot Pr[X=x]
			$$
		The conditional probability for any two discrete random variables $X,Y$ is defined to be 
			$$
			Pr[X=x|Y=y] = \frac{Pr[X=x \cap Y=y]}{Pr[Y=y]}
			$$
		The conditional expectation for any two discrete random variable $X,Y$ is defined to be
			$$
			E[X|Y=y] = \sum_{x} x \cdot Pr[X=x|Y=y]
			$$
		Given the above assertions the proof is with the help of online sources \cite{4,5,6}
			\begin{align*}
				E[E[X|Y]] &= E[\sum_{x} x \cdot Pr[X=x|Y]]\\
				&=\sum_{y} [\sum_{x} x \cdot Pr[X=x|Y=y]] \cdot Pr[Y=y]\\
				&=\sum_{x} x \sum_{y} Pr[X=x|Y=y] \cdot Pr[Y=y]\\
				&=\sum_{x} x \cdot Pr[X=x]\\
				&=E[X]
			\end{align*}
		
		\newpage
		\item \textit{Prove by induction that $E[X_t] = 0$ for a martingale.}\\
		\\
		We want to show that in a martingale sequence the expected value for a random variable $X_{t+1}$ is the random variable $X_t$ before it.\\
		\\
		\textbf{Base case: }\\
		\\
		When $t=0$ with $X_0 = 0$ we have
			\begin{align*}
			E[X_1|X_0] &= \sum_{x} x \cdot Pr[X_1=x|X_0 = 0]\\
			&=0\\
			\end{align*}
		\textbf{Inductive step: }\\
		\\
		We showed that the base case $E[X_1|X_0] = 0$ is true.  Therefore, we can say that $E[X_2|X_1] = E[E[X_1|X0]] = 0$ no matter what $X_2$ is.   This leads to the equation
			$$
			E[X_t|X_0,...,X_{t-1}] = E[E[...E[X_{1}|X_{0}]]] = 0
			$$
		So by induction we have just shown that $E[X_t] = 0$.
		
	\end{enumerate}
	\newpage
	\item \textit{Give a graph $G$ with a source vertex $s$ and a set of edges $E_\pi$ to every vertex in $G$ such that the path from $s$ to $v$ is the shortest
	path but the set of the edges $E_\pi$ cannot be produced by running a breadth-first search on $G$.} \\
	\\
	\begin{figure}[h]
	\caption{Example of a graph where there exists a set of edges that are the shortest path between vertices but will not be found by a breadth first search.}
	\includegraphics[width=6cm]{ps2_q2.png}
	\centering
	\end{figure}
	\\
	For this question, the shortest-path from $s$ to $v$ is defined as the minimum number of edges in any path from vertex $s$ to vertex $v$.  
	If we let the source vertex $s$ be vertex $A$ then the set of edges $E_\pi$ that cannot be produced by running a breadth first search are
	$$
	\{(A,B),(A,C),(B,D),(C,E)\}
	$$
	This is the case because of how breadth first search discovers vertices.  It starts at a source vertex $s$ and then adds every vertex in the adjacency list of $s$ to a queue.  
	In the case above $s$ will be $A$ and it will add $B$ and $C$ to the queue because those are the vertices adjacent to $A$.  
	An edge from $A$ to $B$ and from $A$ to $C$ will be created.  
	Once it does that it will then pop off the first vertex on the queue which will be either $B$ or $C$ depending on the ordering of the adjacency list of $A$.  
	It will then add every vertex from the adjacency list of the vertex is pops off to the queue.
	If either $B$ or $C$ is removed from the queue, $D$ and $E$ are still both added to the queue because there are both in $B$ and $C$ adjacency lists.
	In either case, edges are created from a single vertex, $C$ or $B$, to both $D$ and $E$ i.e. $(C,D),(C,E)$.  It will never be the case that both $C$ and $B$ will have a only single edge i.e. $(C,D),(B,E)$.
	It will only be the case that one of them will have two edges so the set of edges $E_\pi$ above will would never occur with a breadth first search.
	\newpage
	\item \textit{Give an efficient algorithm with an input of a graph $G$ and and an edge $e$ and returns a cut where $e$ is a light edge across the cut or NO if no such cut exists}\\
	\\
	\textbf{Psuedocode}
	\begin{verbatim}
	def LightEdgeCut(G,e):
	    minimum spanning tree = Kruskal's algorithm
	    if e is in minimum spanning tree:
	        create a cut of the graph with e as a light edge
	        return cut
	    else:
	        add e to minimum spanning tree to create a cycle C
	        z = largest edge removed from cycle C
	        if z is e: // Cycle property of minimum spanning tree
	            return NO
	        else:
	            create a cut of the graph with e as a light edge
	            return cut
	\end{verbatim}
	
	\textbf{Correctness}\\
	\\
	The algorithm above depends on two assumptions:
	\begin{enumerate}
		\item If an edge $(u, v)$ is contained in some minimum spanning tree, then it is a light edge crossing some cut of the graph \cite{2}.
		\item For any cycle C in the graph, if the weight of an edge e of C is larger than the individual weights of all other edges of C, then this edge cannot belong to a MST \cite{3}.
	\end{enumerate}
	As an example for hypothesis (a) we cut the the graph of Figure 2 between vertices $A$ and $D$ and vertices $D$ and $E$.  After the cut $S = \{D\}$ and $S'=\{A,B,C,E,F\}$.
	The edge $(A,D)$ is in the minimum spanning tree created by Kruskal's algorithm and it is a light edge of the cut that was made.\\
	\\
	As an example for hypothesis (b) if we add the edge $(D,E)$ to the minimum spanning tree it will create a cycle from vertices $A \rightarrow B \rightarrow E \rightarrow D \rightarrow A$.
	If we remove the largest weighted edge which in this case we say is $(A,D)$ even though it is the same weight as $(D,E)$ we get a new minimum spanning tree that has $(A,D)$ as an edge.  
	If $(A,D)$ had been larger than $(D,E)$ then it would have been removed from the minimum spanning tree and would not be part of any other minimum spanning tree.\\
	\\ 
	We assume Kruskal's algorithm is correct and returns a minimum spanning tree for the graph $G$.  
	It may be that the edge $e$ LightEdgeCut takes as input is part of the minimum spanning tree returned by Kruskal's algorithm.  
	We then know that the edge $e$ is a light edge crossing some cut of $G$ so we can create that cut with $e$ and return it. \\
	\\ 
	If $e$ is not in the minimum spanning tree returned by Kruskal's algorithm then we have to check to see if $e$ is part of any minimum spanning tree of $G$.
	We do this by adding it to the minimum spanning tree returned by Kruskal's algorithm in order to create a cycle.  
	Based on the cycle property of minimum spanning trees the largest edge in a cycle cannot be part of any minimum spanning tree.  If $e$ is the largest edge
	in the cycle we know it is not part of a minimum spanning tree and therefore not a light edge crossing some cut of the graph.  If it is not the largest edge then
	we can conclude it is part of a minimum spanning tree and is a light edge crossing some cut of the graph.\\
	\begin{figure}[h]
	\caption{A graph where the edges of a minimum spanning tree created by Kruskal's algorithm are highlighted in green.}
	\includegraphics[width=8cm]{ps2_q3.png}
	\centering
	\end{figure}
	\\
	\textbf{Runtime}\\
	\\
	Kruskal's algorithm runs in $O(V\,log(E))$.  The worse case of this algorithm will, after going through Kruskal's algorithm, go to the else statement.
	Assuming the minimum spanning tree is a disjoint-set data structure adding the edge $e$ to the spanning tree is a set union operation that takes $O(1)$.
	Finding the edge with the highest weight in the cycle could take $O(V-1)$ if the cycle contains all of the edges in the minimum spanning tree.
	Checking if the largest edge is $e$ takes $O(1)$.  
	Creating a cut of the graph $G$ with $e$ as a light edge takes O(V) if it has to look at all of there vertices where to make the cut.
	The worst case runtime would be $O(V\,log(E)+V+(V-1)) = O(V\,log(E) + V)$.\\
	
	\newpage
	\item \textit{Give a $o(|V|^2)$ algorithm to find a universal sink given an adjacency matrix.}\\
	\\
	\textbf{Pseudocode}\\
	\begin{verbatim}
	FindUniversalSink(A):
	 for k in range(1,|V|):
	  vertex k is a universal sink
	  for i in range(1,|V|):
	   if i != k and A_{ik} == 0: // 0 in the vertex's column that is not a self loop 
	    vertex k not universal sink
	    break
	  for j in range(1,|V|):
	   if A_{kj} == 1: // 1 in the vertex's row
	    vertex k not universal sink
	    break
	 if vertex k is still a universal sink:
	  return vertex k
	 elif k == |V|:
	  return NO
	            
	               
	            	
	                
	       
	\end{verbatim}
	\textbf{Correctness}\\
	\\
	In order for this algorithm to be correct I make the following assumptions about an adjacency matrix:
	\begin{enumerate}
		\item If a graph is represented as an adjacency matrix $A$ then $A_{ij}$ is the edge from vertex $i$ to vertex $j$. \\ 
		\item If a 1 is present in the matrix at row $i$ column $j$ then an edge exists from $i$ to $j$.  If a 0 is present then no edge exists from $i$ to $j$.\\
		\item A universal sink will have 0's in its entire row of the adjacency matrix meaning there are no edges from it to any other vertex or to itself in the form of a self-loop.\\  
		\item Every vertex, not including the universal sink, will have a 1 in the column the belongs to the universal sink.  This means that they all have edges from themselves to the universal sink.\\
		\item A vertex that has a 1 in any part of its row in the matrix or a 0 in any part of its column in the matrix other then the cell that is a self-loop then it is not a universal sink.\\
	\end{enumerate}
	I claim that FindUniversalSink will always return a universal sink if there is one to be found in the adjacency matrix $A$ otherwise NO is returned.  
	It does this by looking at and assuming every vertex in the adjacency matrix is the universal sink until proven otherwise.\\
	\\
	The algorithm begins with a adjacency list.  It continues by iterating vertex by vertex through the adjacency list, setting each vertex as the universal sink.  
	Each time a new vertex is set as the universal sink it first looks at the entire column of the vertex and checks to see if there is a 0 in any place other than the self loop cell where row number $i$ equals column number $j$.     
	This indicates this vertex is not a universal sink because it does not have an edge from every other vertex in the graph going to it.
	The algorithm then looks at the row of the vertex to see if there are any 1's in any place.  This also indicates this vertex is not a universal sink because a universal sink does not have any out-edges.
	If it is found that this vertex is not a universal sink by either looking at the column or the row of the vertex, the algorithm sets the vertex to not being a universal sink and breaks out of whichever loop it is currently and 
	ends up at a conditional statement.\\
	\\
	This conditional statement checks if the vertex is still a universal sink.  If it is a universal sink the vertex is returned otherwise two things could happen.  
	We could be at the last vertex in the adjacency list which, if we have reached this point, is not a universal sink so there exists on universal sink the graph.
	Or we could not be at the last vertex in the adjacency list which means we will go back to the top loop and move to and check the next vertex in the adjacency list.
	
	\textbf{Runtime}\\
	Unfortunately, I was unable to to create an algorithm that had a runtime of $o(|V|^2)$.
	The first loop of FindUniversalSink runs from $|V|$ times as it goes through all of the vertices.  
	With in that loop, there are two other loops that also run through all of the vertices in the adjacency graph so they run a total of $2|V|$ times.
	The rest of the operations like checking the values in the cells of the matrix or seeing if a vertex is a universal sink run in in constant time.\\
	\\
	This means this algorithm has a runtime of $O(|V|*2|V|) = O(|V|^2)$.  
	
	\newpage
	\item
		\begin{enumerate}
			\item \textit{Prove that every tree is a bipartite graph.}\\
			\\
			\begin{figure}[h]
			\caption{An example of a bipartite graph.}
			\includegraphics[width=8cm]{ps2_q5a.png}
			\centering
			\end{figure}
			\\
			\begin{figure}[h]
			\caption{An example of a non bipartite graph.}
			\includegraphics[width=8cm]{ps2_a5aa.png}
			\centering
			\end{figure}			
			\begin{figure}[h]
			\caption{An example of a tree.}
			\includegraphics[width=8cm]{ps2_q5aaa.png}
			\centering
			\end{figure}
			\newpage
			If you notice in the pictures above a graph that has an even number of vertices in a cycle (an even cycle) is bipartite while a graph that has an odd number of vertices in a cycle (an odd cycle) is not bipartite \cite{1}.
			A tree is a connected, acyclic, and undirected graph.  Since it does contain any cycles, in particular an odd cycle, I claim it must be bipartite.\\
			\\
			If we were to assume a tree was not bipartite then no matter how the vertices were partitioned there would always be at least one edge $(u,v)$ of the tree where $u$ and $v$ were in the 
			same vertex subset if the vertices were broken into two susbets.  
			Starting with two vertices in a tree which is the minimum number of vertices to form an edge those vertices can be partitioned into two subsets that will make the tree bipartite.  
			The edge $(u,v)$ between the two vertices will have one end in one subset and the other end in the other subset.\\
			\\
			With three or more vertices in a graph, the only way to make a graph not bipartite is to create cycles between the vertices as in Figure 4.  But as we see in Figure 3, a graph with an even cycle is still bipartite.  
			This would mean a tree would need to have an odd cycle to not be bipartite but because a tree by definition will never have any cycles it will always be bipartite.\\    
			\newpage
			\item \textit{Given an efficient alogrithm to determine whether a given undirected graph is bipartite.}\\
			\\
			\textbf{Correctness}\\
			\\
			We can determine if an undirected graph is bipartite by coloring the vertices in the graph with two different colors \cite{7}.
			We can search through the graph and when we hit one endpoint of an edge we color it one color and when we hit the next endpoint of the edge we color it a different color.
			We will end up with all the nodes in $R$ being a different color compared to the color of all of the nodes in $L$ if the graph is bipartite.
			To do this we can use a modified breadth first search.\\
			\\
			When we run breadth first search we first take the source vertex that is in the input to the algorithm and color it one particular color--lets say red for this example.
			Then we put it on the queue.  When the source is removed from the queue we check to make sure its color is correct and then we look at all of its neighbors.  
			To figure out which color to use for the neighbors of any vertex we look at the distance from the the source vertex to each neighbor vertex.  
			In the example of the neighbors of the source vertex they will all be a distance of one away from the source so we color them green.  The next vertices from the source's neighbors will
			be a distance of two away so we will color them red.  To generalize, any vertex that is an even distance away is colored red and any vertex that is an odd distance away from the source
			will be colored green.  If we pop a vertex from the queue and check its color and it disagrees with the coloring scheme we know that this graph is not bipartite.\\			
			\\
			Since breadth first search discovers nodes one level of distance away from the source vertex at a time this change to the algorithm ensures 
			that if the graph is bipartite the vertices will be colored in a way that leaves each endpoint of an edge a different color.
			This means that vertices of color one can be partitioned into one subset $R$ and vertices of color two can be partition into another subset $L$.  
			These subsets will have one endpoint of an edge in $R$ and one endpoint of an edge in $L$.\\
			\\
			\textbf{Runtime}\\
			\\
			The regular breadth first search algorithm runs in $O(V+E)$.  
			We are adding a color attribute to a node which is just another array and inserting and checking the values of the array takes O(1) time per node.  
			I claim that the runtime of the modified breadth first search remains $O(V+E)$.
		\end{enumerate}
		
		\item
			\begin{enumerate}
				\item \textit{Give an example graph with a unique MST and two edges of equal length.  Give an algorithm to construct a graph with $\Omega{(|V|^2)}$
				edges of equal weight yet still having a unique MST.}\\
				\\
				\textbf{Example Graph}\\
				\\
				\begin{figure}[h]
				\caption{An example of a unique MST with two edges of equal length.}
				\includegraphics[width=8cm]{ps2_q6a.png}
				\centering
				\end{figure}
				\\
				In Figure 6 above, the MST of the graph will always have the set of edges $\{(A,B),(A,C)\}$ since those are the minimum weight edges connecting all the nodes in the graph.\\
				\\
				\textbf{Algorithm}
								
				\item \textit{Refute the following two claims about a unique MST:}
					\begin{enumerate}
						\item \textit{Every cut has a unique minimum weight across it.}\\
						\\
						We can use Figure 6 to show that this claim is not true.  
						If we cut the graph between vertices $A$ and $B$ and $A$ and $C$ where $S = \{A\}$ and $S' =\{B,C\}$ we see that both edges $(A,B)$ and $(A,C)$
						are light edges that cross the cut and so there is not a unique minimum weight across it.\\
						\item \textit{The maximum weight edge in any cycle of a graph is unique.}\\
						\\
						\begin{figure}[h]
						\caption{An example of a graph with a unique MST and maximum weight edge in a cycle that is not unique.}
						\includegraphics[width=8cm]{ps2_q6bii.png}
						\centering
						\end{figure}
						\\
						The MST in Figure 7 consists of the set of edges $\{(A,B),(A,D),(A,C)\}$.  The cycle in Figure 7 consists of the set of edges $\{(A,B),(B,C),(C,D),(D,A)\}$.  
						As can be seen, the edges $(B,C)$ and $(C,D)$ have the same maximum weight in the cycle so the claim that the maximum weight edge in any cycle of a graph is unique is false.  
					\end{enumerate}
			\end{enumerate}
	
\end{enumerate}

\begin{thebibliography}{10}
	\bibitem{1} https://en.wikipedia.org/wiki/Cycle\_graph\\
	\bibitem{2} https://en.wikipedia.org/wiki/Minimum\_spanning\_tree\\
	\bibitem{3} http://test.scripts.psu.edu/users/d/j/djh300/cmpsc465/notes-4985903869437/solutions-to-some-homework-exercises-as-shared-with-students/4-solutions-clrs-23.pdf\\
	\bibitem{4} http://www.maths.qmul.ac.uk/~pettit/MTH5122/notes15.pdf\\
	\bibitem{5} https://en.wikipedia.org/wiki/Expected\_value\\
	\bibitem{6} https://math.stackexchange.com/questions/1353418/expected-value-proof-law-of-total-expectation\\
	\bibitem{7} https://en.wikipedia.org/wiki/Bipartite\_graph
\end{thebibliography}

\end{document}