\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage[linesnumbered,lined,boxed,commentsnumbered]{algorithm2e}
\begin{document}
\title{CSCI 5454: PS1}
\author{Robert Werthman}
\date{}
\maketitle

\section*{1.}

\subsection*{}
Let's say these algorithms solve an array sorting problem.\\
\begin{itemize}
\item Let algorithm $A$ be bubblesort with a worst-case runtime of $n^2$.\\
\item Let algorithm $B$ be mergesort with a worst-case runtime of $n*log(n)$.\\
\item Let $C$ be the newly designed sorting algorithm with a worst-case runtime of $h(n)$.\\
\end{itemize}
In this case, $O(min(f(n),g(n)))$ will become $O(n*log(n))$ because it is the smaller of the two runtimes.\\
If $h(n)$ is $log(n)$ then $h(n)$ achieves the running time $O(min(f(n),g(n)))$ because $log(n)$ does not grow faster than $n*log(n)$ and is therefore bounded above by it.\\

\subsection*{}
Yes, you can achieve a running time exactly $min(f(n),g(n))$. Algorithm $C$ would need to be designed in such a way that its running was equal to $min(f(n),g(n))$.\\

\section*{2.}

\subsection*{}
\textbf{Proposition/Claim: } For any real constants $a$ and $b$, where $b > 0$, the asymptotic relation $(n+a)^b = \Theta(n^b)$ is true.\\
\\
\textbf{Theorem: }The asymptotic relation $(n+a)^b = \Theta(n^b)$ is true iff:
\begin{itemize}
\item There exists positive constants $c_1, c_2, n_0$ s
uch that $0 \le c_1(n^b) \le (n+a)^b \le c_2(n^b)$ for all $n \ge n_0$.
\end{itemize}In order to prove the proposition above we must find some constants $c_1, c_2, n_0$ to satisfy the above bulleted sentence.\\
\\
\textbf{Proof: }\\
First we want to find the floor and ceiling of $n+a$ so we can create an inequality similar to the one in the theorem above.
\begin{enumerate}
\item If $|a| \le n$ then we can say that $n+a \le n+|a| \le 2n$ (Ceiling of $n+a$).
\item If $|a| \le \frac{1}{2}n$ then we can say that $n+a \ge n-|a| \ge \frac{1}{2}n$ (Floor of $n+a$). 
\end{enumerate}
Now if $2|a| \le n$ then we can combine the floor and ceilings into an compound inequality that holds true :
$$
0 \le \frac{1}{2}n \le n+a \le 2n
$$
The only thing missing from this new equation is a power of $b$.  Raising the new equation to a power of $b$ gives:
$$
0 \le (\frac{1}{2}n)^b \le (n+a)^b \le (2n)^b \Rightarrow 0 \le (\frac{1}{2})^bn^b \le (n+a)^b \le (2)^bn^b
$$  
Extracting the constants $c_1,c_2,n_0$ from this equation yields $c_1 = (\frac{1}{2})^b$, $c_2 = 2^b$, and $n_0 = 2|a|$ since $n \ge 2|a|$.  These represent one solution.
\section*{3.}
$f(n) = \Omega{g(n)}$ means that for all values to the right of some $n_0$ the value of $f(n)$ is on or above $cg(n)$.\\
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
$n!$&$e^n$&$(\frac{3}{2})^n$&$(lg\,n)!$&$n^2$&$n\,lg\,n$&$lg(n!)$&n&$(\sqrt{2})^{lg\,n}$&$2^{lg*n}$&$n^{1/lg\,n}$&1\\
\hline
\end{tabular}
\end{center}
\subsection*{Equivalence Classes}
$lg(n!) = \Theta(n\,lg\,n)$\\
$n^{1/lg\,n} = \Theta(1)$

\section*{4.}
\subsection*{a.} $T(n) = T(n-1)+n,\,T(1) = 1$\\
I will use a recurrence tree to solve this recurrence relation.\\
\begin{center}
\begin{tikzpicture}
\node (z) {$n$}
child {node (a) {$n-1$} 
child {node (b) {$n-2$}
child {node (c) {$2$}
child {node (d) {$\vdots$}
child {node (e) {$1$}}
}
}
}
};
\end{tikzpicture}
\end{center}
Tree depth = n\\
Cost per level = i\\
So $T(n) = \sum_{i = 1}^{n} i = \frac{n(n+1)}{2} = \frac{n^2}{2} + \frac{n}{2}$\\
Therefore, it can be said that $T(n) =  O(n^2)$\\
\subsection*{b.} $T(n) = 2T(n/2)+n^3,\,T(1) = 1$\\
I will use the master method to solve this recurrence relation.\\
$a=2, b=2, f(n)=n^3$\\
so $n^{\log_{b} a} = n^{\log_{2} 2} = n$\\
This tells us that the first 2 rules of the master theorem do not apply.
\begin{enumerate}
\item $f(n) \ne O(n^{1-\epsilon})$
\item $f(n) \ne \Theta{(n)}$
\end{enumerate}
This leaves the 3rd rule of the master theorem as the solution.\\
\begin{enumerate}
\setcounter{enumi}{2}
\item $f(n) = n^3 = \Omega{(n^{1+\epsilon})}$ if $\epsilon = 1$. \\ 
And $2f(n/2) \le cf(n) \Rightarrow 2(n/2)^3 \le cn^3$ if $c=\frac{1}{2}$ and $n \ge 1$.\\
\end{enumerate}
Therefore, $T(n) = \Theta{(n^3)}$.

\section*{5.}
\subsection*{a.}
\begin{algorithm}
\KwData{Nearly sorted array of size n integers}
\KwResult{Completely sorted array}
\BlankLine
\For {j = 2 to A.length}{
	key = A[j]\;
	i = j - 1\;
	\While {i $>$ 0 and A[i] $>$ key}{
		A[i+1] = A[i]\;
		i = i - 1\;
	}
	A[i+1] = key\;
}
\caption{Insertion-Sort(A)}
\end{algorithm}
\textbf{Analysis: }In order to figure out the running time of Insertion Sort we need to add up the cost of each statement in the algorithm.\\
\begin{itemize}
\item If the array is of size n then the statement \textbf{for j = 2 to A.length} will execute $n$ times with a cost of $c_1$.\\
\item The statements \textbf{key = A[j]} (inserting into an array) and \textbf{i=j-1} (setting a variable) will execute $n-1$ times each with a cost of $c_2$ and $c_3$ respectively.\\
\item Since $k$ elements are unsorted in this array than any unsorted element is no more than $k$ places away from its sorted position.  This means that the statement \textbf{while i $>$ 0 and A[i] $>$ key} could be executed in the worst case $\sum_{j=2}^{n} k$ times with a cost of $c_4$.\\
\item The statements \textbf{A[i+1] = A[i]} (inserting into an array) and \textbf{i = i + 1} (setting a variable) are executed $\sum_{j=2}^{n} k - 1$ times with a cost of $c_5$ and $c_6$ respectively.\\
\item Finally, the statement \textbf{A[i+1] = key} (inserting into an array) is executed $n-1$ times with a cost of $c_7$.\\
\end{itemize}
Therefore, the equation for the runtime, $T(n)$, of insertion-sort is:\\
\begin{align*}
T(n)  & = c_1n + c_2(n-1) + c_3(n-1) + c_4(\sum_{j=2}^{n} k) + c_5(\sum_{j=2}^{n} k - 1) + c_6(\sum_{j=2}^{n} k - 1) + c_7(n-1)\\
	& = c_1n + c_2(n-1) + c_3(n-1) + c_4(k(n-1)) + c_5(\sum_{j=2}^{n} k - 1) + c_6(\sum_{j=2}^{n} k - 1) + c_7(n-1)
\end{align*}
Since $k < n$ further reduction of $T(n)$ would yield a linear function of $n$ so we can say the runtime would turn out to be $O(n)$.

\subsection*{b.}
The sorting algorithm I suggest to get a $O(n)$ runtime is Counting Sort.
\begin{algorithm}
\KwData{A is the input array of length $n$}
\KwData{B is the sorted array of length $n$}
\KwData{$k$ is the highest integer in A}
\BlankLine
let $C[0..k]$ be a new array\\
\For{$i=0$ to $k$}{
	$C[i] = 0$\\
}
\For{$j=1$ to $A.length$}{
	$C[A[j]] = C[A[j]+1$\\
}
\For{$i=1$ to $k$}{
	$C[A[j]] = C[i] + C[i-1]$\\
}
\For{$j = A.length$ downto 1}{
	$B[C[A[j]]] = A[j]$\\
	$C[A[j]] = C[A[j]] - 1$\\
}
\caption{Counting-Sort(A, B, $k$)}
\end{algorithm}
\\
\textbf{Analysis: }
\begin{itemize}
\item Initializing $C[0..k]$ takes $k+1$ time to execute and costs $c_0$.
\item The statement \textbf{for i = 0 to k} takes $k+1$ times to execute and costs $c_1$.
\item The statements \textbf{for j = 1 to A.length} and \textbf{j = A.length downto 1} take $n$ times to execute and cost $c_3$ and $c_4$ respectively.
\item The statement \textbf{i = 1 to k} takes $k$ times to execute and costs $c_2$.
\end{itemize}
The equation for the runtime, $T(n)$, of Counting Sort is:\\
$$
T(n) = c_0(k+1) + c_1(k+1) + c_3n + c_4n + c_2k\ldots
$$

\noindent
Reducing $T(n)$ further would show that the runtime of Counting Sort is a linear function of $n$ that runs in a linear time of $O(k+n)$.
If $k=O(n)$ then the running time is $\Theta(n)$.

\subsection*{c.}
(b) doesn't contradict the $\Omega{(n\,log\,n)}$ lower bound given on page 59 of the textbook because the algorithm is not a comparison sorting algorithm.\\
It has been proven that any comparison sort must make $\Omega{n\,log\,n)}$ comparisons in the worst case to sort $n$ elements.  Since counting sort is not a comparison sorting algorithm its runtime is not bounded by $\Omega{(n\,log\,n)}$.

\section*{6}
\textbf{Lemma 1: }  A good minion tells the truth.\\
\textbf{Lemma 2: } A bad minion could be telling the truth or could be lying.\\
\\
Let $g$ be the number of good minions, $b$ be the number of bad minions, and $n$ be the total number of minions.\\
\\
\subsection*{a.}
\textbf{Proposition/Claim :} 
If $n/2$ or more minions are bad, Gru cannot necessarily determine which minions are good.\\
\\
\textbf{Proof: }  This claim is proven by analyzing the cases.\\
\\
The comparison in the chamber can be between two good minions, two bad minions, or one good and one bad minion.\\
\\
The claim assumes that $b \ge n/2$.\\
\\
\textbf{Case 1: } Two good minions size each other up.\\
The result of that comparison would be:\\
\begin{center}
\begin{tabular}{c|c}
Minion A & Minion B\\
\hline
good & good\\
\end{tabular}
\end{center}
\textbf{Case 2: } Two bad minions size each other up.\\
The result of the comparison could be:
\begin{center}
\begin{tabular}{c|c}
Minion A & Minion B\\
\hline
good & good\\
bad & bad\\
good & bad\\
bad & good\\
\end{tabular}
\end{center}
\textbf{Case 3: } One good minion and one bad minion size each other up.\\
The result of the comparison if A was good and B was bad could be: 
\begin{center}
\begin{tabular}{c|c}
Minion A & Minion B\\
\hline
good & bad\\
bad & bad\\
\end{tabular}
\end{center}
The result of the comparison if A was bad and B was good could be: 

\begin{center}
\begin{tabular}{c|c}
Minion A & Minion B\\
\hline
bad & good\\
bad & bad\\
\end{tabular}
\end{center}

\noindent
\textbf{Analysis: }As can be seen from the cases above, two bad minions sizing each other up in the chamber 
can lead to the same results as two good minions or one good minion and one bad minion sizing each other up in the chamber.
\\
Gru has no way to tell if the results he is seeing in the chamber are from two good minions, two bad minions, or one bad and one good minion. 

\subsection*{b.}
\textbf{Proposition/Claim :} 
$n/2$ pairwise tests are sufficient to reduce the problem of finding a single good minion to one of nearly half the size.\\
\\
The claim assumes that $g > n/2$.\\
\\
\textbf{Proof: }\\
$n/2$ pairwise tests means that if $n$ is even, like the number 50, then all minions are tested against another minion.  For example, if there are 50 minions then they would be split into 25 ($50/2$) pairs of minions that will be sent to the chamber to be tested. 
If $n$ is odd, like the number 51, then one minion will be left out of the initial $n/2$ pairwise tests.  For example, if there are 51 minions they would be split into 25 ($50/2$) pairs of minions with one minion left over and not being paired up.\\
\\
In order to reduce the problem by nearly half the size we will have to get rid of minions and to do that we need to look at the four possible outcomes of a test.
Of the four possible outcomes of minions sizing up each other in the chamber, three have the conclusion that at least one minion in the chamber is bad.
With this in mind, only keeping the minions from the outcome that resulted in minion A saying "good" and minion B saying "good" would result in at most one good minion and at least one bad minion being removed for each of the tests that resulted in the other outcomes.
Because we are getting rid of the minions of 3/4 of the possible outcomes of a test the size of the problem, the number of minions, is reduced by at least half if a good minions are tested with bad minions.
Additionally, since we get rid of one bad minion with every good minion and more than $n/2$ of the minions are good we know that there will be at least one good minion in the remaining group after the pairwise tests.

\subsection*{c.} 
\textbf{Proposition/Claim: } 
Good minions can be identified with $\Theta{(n)}$ pairwise tests, assuming more than $n/2$ of the minions are good.\\
\\
\textbf{Proof: }
The problem can be categorized as a recurrence starting with $n$ as the initial size of the problem which is the initial number of minions.  $n$ is then continuously divided by 2 each pairwise test iteration as section b states above.  We can then create a recurrence equation below which describes this minion testing procedure below:
\begin{equation}
T(n) = T(n/2) + n
\end{equation}

\noindent
Part 3 of the master method can be used to solve this recurrence relation.
$f(n) = n, b = 2, a = 1$
\begin{itemize}
\item $f(n) = n = \Omega{(n^{0+\epsilon})}$ where $\epsilon$ is .5
\item $n/2 \le cn$ where $c = .9$ and all sufficiently large n
\end{itemize}
Therefore, $T(n) = \Theta{(n)}$ which proves that a minion can be identified with $\Theta{(n)}$ pairwise tests.
\end{document}  